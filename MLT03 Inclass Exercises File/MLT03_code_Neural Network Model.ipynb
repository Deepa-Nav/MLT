{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLT-03 ANN Concepts\n",
    "\n",
    "- Authored by: *Jay Parmar*\n",
    "- Last modified on: *27th August 2023*\n",
    "\n",
    "## In this notebook, we will go through following topics:\n",
    "\n",
    "- Feature Engineering\n",
    "- Neural Network Creation\n",
    "- Strategy Backtesting\n",
    "\n",
    "The focus of this lab would be to understand the implementation of neural networks using the scikit-learn library. To do so, we will start with importing necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Training a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Apply the default seaborn theme, scaling, and color palette\n",
    "sns.set()\n",
    "# One can use different colour palettes\n",
    "# palettes = [\"deep\", \"muted\", \"pastel\", \"bright\", \"dark\", \"colorblind\"]\n",
    "# sns.set(palette=\"deep\")\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data from the local file\n",
    "df = pd.read_csv('EURUSD_data.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the original dataframe. Will work on the new dataframe.\n",
    "data = df.copy()\n",
    "# Checking the shape\n",
    "print('Number of observations:', data.shape[0])\n",
    "print('Number of variables:', data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating features\n",
    "features_list = []\n",
    "\n",
    "# SD based features\n",
    "for i in range(5, 20, 5):\n",
    "    col_name = 'std_' + str(i)\n",
    "    data[col_name] = data['Close'].rolling(window=i).std()\n",
    "    features_list.append(col_name)\n",
    "    \n",
    "# MA based features\n",
    "for i in range(10, 30, 5):\n",
    "    col_name = 'ma_' + str(i)\n",
    "    data[col_name] = data['Close'].rolling(window=i).mean()\n",
    "    features_list.append(col_name)\n",
    "    \n",
    "# Daily pct change based features\n",
    "for i in range(3, 12, 3):\n",
    "    col_name = 'pct_' + str(i)\n",
    "    data[col_name] = data['Close'].pct_change().rolling(i).sum()\n",
    "    features_list.append(col_name)\n",
    "    \n",
    "# Intraday movement\n",
    "col_name = 'co'\n",
    "data[col_name] = data['Close'] - data['Open']\n",
    "features_list.append(col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, We'll use popular technical indicators to build features. They are as follows:\n",
    "\n",
    "- [Bollinger Bands](https://en.wikipedia.org/wiki/Bollinger_Bands)\n",
    "- [Moving Average Convergence/Divergence (MACD)](https://en.wikipedia.org/wiki/MACD)\n",
    "- [Parabolic Stop And Reverse (SAR)](https://en.wikipedia.org/wiki/Parabolic_SAR)\n",
    "\n",
    "\n",
    "The discussion about what these technical indicators and how they are built, is out of scope of this session. We'll use `TA-LIB` library to build these indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following command on the terminal window on Anaconda to install ta-lib if it is not installed\n",
    "# conda install -c conda-forge ta-lib\n",
    "import talib as ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['upper_band'], data['middle_band'], data['lower_band'] = ta.BBANDS(data['Close'].values)\n",
    "data['macd'], data['macdsignal'], data['macdhist'] = ta.MACD(data['Close'].values)\n",
    "data['sar'] = ta.SAR(data['High'].values, data['Low'].values)\n",
    "features_list +=['upper_band','middle_band','lower_band','macd','sar']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML algorithms don't work with `NaN` values. However, while creating the above features, we would have many `NaN` values we need to drop from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features_list].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are null values in many columns. Let's drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[features_list].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As OHLC data is high correlated, we won't be using them as features. Instead, we would use only technical indicators and quantitative features for this exercise. Below we define feature matrix `X`, create the target variable and assign it to the target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[features_list]\n",
    "data['target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `train_test_split()` function from the `sklearn.model_selection` package to split our dataset. We will use 20% of our dataset as a test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can train our neural network, we need to make sure that our data is scaled, that is, it ranges between 0 and 1. We will use `StandardScaler` from the `sklearn.preprocessing` package. We need to train the scaler object on training data only and then apply on training and testing set both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "#sns.pairplot(X_train_scaled_df[features_list]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything ready now. Now is the time to create our first neural network. We'll use the `MLPClassifier` from the `sklearn.neural_network` package. Here, MLP stands for Multi Layer Perceptron. A simple neural network is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://www.learnopencv.com/wp-content/uploads/2017/10/mlp-diagram-600x400.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a neural network consists of\n",
    "\n",
    "- Input layer,\n",
    "- Hidden layer, and \n",
    "- Output layer\n",
    "\n",
    "Hence, we need to define these layers for the model. In our case, the feature matrix `X` becomes input to the input layer. Then we have hidden layer/s and finally the output layer. In the above diagram, we have only one hidden layer.\n",
    "\n",
    "    Note: In `sklearn` library, we need not specify the size of the input and output layer. It will be fixed by the library itself when we train it. Hence, we are only to define hidden layer sizes.\n",
    "\n",
    "Below we define the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "# model = MLPClassifier(hidden_layer_sizes=(5), verbose=True, random_state=10)\n",
    "model = MLPClassifier(hidden_layer_sizes=(5), max_iter=300, activation = 'tanh', solver='adam', random_state=1, shuffle=False)\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, we have successfully trained our first neural network. Now let's check its properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of layers in the model\n",
    "model.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check weights\n",
    "print('Weights between input layer and the hidden layer:')\n",
    "print(model.coefs_[0])\n",
    "print('Biases between input layer and the hidden layer:')\n",
    "print(model.intercepts_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Weights between hidden layer and the output layer:')\n",
    "print(model.coefs_[1])\n",
    "print('Biases between hidden layer and the output layer:')\n",
    "print(model.intercepts_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model accuracy on training data\n",
    "print('Model accuracy on training data:', model.score(X_train_scaled, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model accuracy on testing data\n",
    "print('Model accuracy on testing data:', model.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict data\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As training and testing accuracy are very similar, we can consider that model might not have overfitted, and it may generalize well. However, it is difficult to claim until we evaluate the model properly.\n",
    "\n",
    "Also, the model that we have created is a very simple one; we have used most of the default parameters for building the model. And they might not be the best one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Precision and Recall\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Backtesting our predictions\n",
    "\n",
    "So far we've covered\n",
    "* Read Data\n",
    "* Create Features\n",
    "* Scale data\n",
    "* Use already trained model to make predictions\n",
    "* Trade on those prediction, and calculate the strategy returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(df, model):\n",
    "    # Copy data\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Create returns\n",
    "    data['returns'] = np.log(data['Close'] / data['Close'].shift(1))\n",
    "    # Creating features\n",
    "    features_list = []\n",
    "\n",
    "    # SD based features\n",
    "    for i in range(5, 20, 5):\n",
    "        col_name = 'std_' + str(i)\n",
    "        data[col_name] = data['Close'].rolling(window=i).std()\n",
    "        features_list.append(col_name)\n",
    "\n",
    "    # MA based features\n",
    "    for i in range(10, 30, 5):\n",
    "        col_name = 'ma_' + str(i)\n",
    "        data[col_name] = data['Close'].rolling(window=i).mean()\n",
    "        features_list.append(col_name)\n",
    "\n",
    "    # Daily pct change based features\n",
    "    for i in range(3, 12, 3):\n",
    "        col_name = 'pct_' + str(i)\n",
    "        data[col_name] = data['Close'].pct_change().rolling(i).sum()\n",
    "        features_list.append(col_name)\n",
    "\n",
    "    # Intraday movement\n",
    "    col_name = 'co'\n",
    "    data[col_name] = data['Close'] - data['Open']\n",
    "    features_list.append(col_name)\n",
    "    # Create features\n",
    "    data['upper_band'], data['middle_band'], data['lower_band'] = ta.BBANDS(data['Close'].values)\n",
    "    data['macd'], data['macdsignal'], data['macdhist'] = ta.MACD(data['Close'].values)\n",
    "    data['sar'] = ta.SAR(data['High'].values, data['Low'].values)\n",
    "    features_list +=['upper_band','middle_band','lower_band','macd','sar']\n",
    "    # Create target\n",
    "    data['target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "    \n",
    "    # Drop null values\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    # Create feature matrix and target vector\n",
    "    X = data[features_list]\n",
    "    y = data['target']\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model.predict(X_scaled)\n",
    "    \n",
    "    data['predicted'] = y_pred\n",
    "    \n",
    "    # Create strategy returns\n",
    "    data['strategy_returns'] = data['returns'].shift(-1) * data['predicted']\n",
    "    \n",
    "    # Return the last cumulative return\n",
    "    bnh_returns = data['returns'].cumsum()[-1]\n",
    "    \n",
    "    # Return the last cumulative strategy return\n",
    "    # we need to drop the last nan value\n",
    "    data.dropna(inplace=True)\n",
    "    strategy_returns = data['strategy_returns'].cumsum()[-1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data['returns'].cumsum())\n",
    "    plt.plot(data['strategy_returns'].cumsum())\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Cumulative Returns')\n",
    "    plt.title('Returns Comparison')\n",
    "    plt.legend([\"Buy and Hold Returns\",\"Strategy Returns\"])\n",
    "    plt.show()\n",
    "    \n",
    "    return bnh_returns, strategy_returns, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read backtest data\n",
    "backtest_data = pd.read_csv('EURUSD_backtest.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "# Backtest the strategy\n",
    "bnh_returns, s_returns, data = backtest(backtest_data, model)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Buy and Hold Returns:', bnh_returns)\n",
    "print('Strategy Returns:', s_returns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
